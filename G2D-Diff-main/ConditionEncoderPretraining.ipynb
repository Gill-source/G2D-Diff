{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5923b2-e4ea-4fdd-a648-f5c59a9650d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn \n",
    "import random\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from src.utils.g2d_diff_genodrug_dataset import *\n",
    "from src.g2d_diff_ce import *\n",
    "from ignite.handlers.param_scheduler import create_lr_scheduler_with_warmup\n",
    "\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f89b78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e39532-9197-4add-8e4c-6f6a6a1ed1ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d911e7-8fb9-47b8-b164-b8d6314e64cc",
   "metadata": {},
   "source": [
    "## CE Pretraining Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31668bc-ca72-4ba3-9680-960606aebd4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import datetime\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "import copy\n",
    "\n",
    "\n",
    "class G2D_DIFF_CE_Pretrain(nn.Module):\n",
    "    def __init__(self, num_of_genotypes = 1, num_of_dcls = 5, cond_dim = 128,  drug_dim = 128, use_nest_info = True,  device = 'cuda'):\n",
    "        super(G2D_DIFF_CE_Pretrain, self).__init__()\n",
    "        \n",
    "       \n",
    "\n",
    "        self.cond_mapper = nn.Linear(cond_dim, cond_dim)\n",
    "        self.drug_mapper = nn.Linear(drug_dim, cond_dim)\n",
    "        self.drug_encoder = DrugEncoder(input_dim = drug_dim,  device = device)\n",
    "\n",
    "        ## If you want to get attention results, pass get_att = True here\n",
    "        self.condition_encoder = Condition_Encoder(num_of_genotypes=num_of_genotypes, num_of_dcls=num_of_dcls, \\\n",
    "                                                   device = device, neighbor_info = use_nest_info, get_att = False)\n",
    "          \n",
    "        \n",
    "        \n",
    "        self.num_of_genotypes = num_of_genotypes\n",
    "        self.num_of_dcls = num_of_dcls\n",
    "        self.condim = cond_dim\n",
    "        self.device_name = device\n",
    "        \n",
    "        \n",
    "       \n",
    "\n",
    "        \n",
    "    def train(self, dataset_obj, collate_fn, train_config: Dict, sampler):\n",
    "\n",
    "        num_neg = train_config['num_neg']\n",
    "        warmup_epoch = train_config['warmup_epoch']\n",
    "        temperature = train_config['cons_temp']\n",
    "        lr = train_config['lr']\n",
    "        batch_size = train_config['batch_size']\n",
    "        epochs = train_config['epoch']\n",
    "        max_step = train_config['max_step']\n",
    "        train_config['current_epoch'] = 0\n",
    "        seed = train_config['seed']\n",
    "        \n",
    "        \n",
    "        sim_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "        \n",
    "        C_solver = optim.Adam(list(self.cond_mapper.parameters())+list(self.drug_mapper.parameters())+\\\n",
    "                              list(self.condition_encoder.parameters())+list(self.drug_encoder.parameters()),lr=lr)\n",
    "        \n",
    "           \n",
    "        lr_scheduler = create_lr_scheduler_with_warmup(optim.lr_scheduler.LambdaLR(C_solver, lr_lambda=[lambda epoch: 1]),\n",
    "                                               warmup_start_value=0.0,\n",
    "                                               warmup_duration= warmup_epoch * max_step,\n",
    "                                               warmup_end_value=lr)\n",
    "        fin_c_losses = []\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        tr_loader = DataLoader(dataset_obj, batch_size=batch_size, drop_last=True, collate_fn=collate_fn, sampler = sampler)\n",
    "        \n",
    "        for epoch in range(train_config['current_epoch'], epochs):\n",
    "            \n",
    "            C_losses = []\n",
    "            print (\"Epoch: %d\" %epoch)\n",
    "            \n",
    "            \n",
    "            for i, batch in tqdm(enumerate(tr_loader), total = max_step):\n",
    "                lr_scheduler(None)\n",
    "                \n",
    "                ## Batch data load to device\n",
    "                for key in batch.keys():\n",
    "                    if 'genotype' in key:\n",
    "                        for mut in batch[key].keys():\n",
    "                            batch[key][mut] = batch[key][mut].to(self.device_name)\n",
    "                    elif key == 'cell_name':\n",
    "                        None\n",
    "                    elif key == 'drug_name':\n",
    "                        None\n",
    "                    else:\n",
    "                        batch[key] = batch[key].to(self.device_name)\n",
    "   \n",
    "                ## Final gene embeddings, Final cond encoding, Final layer attention, Whole attention list\n",
    "                _, cond_orig, _, _ = self.condition_encoder(batch)\n",
    "                drug_orig = self.drug_encoder(batch['drug'])\n",
    "            \n",
    "                cond_feats = self.cond_mapper(cond_orig)\n",
    "                drug_feats = self.drug_mapper(drug_orig)\n",
    "\n",
    "                cfeat_norm = nn.functional.normalize(cond_feats, dim = -1)\n",
    "                dfeat_norm = nn.functional.normalize(drug_feats, dim = -1)\n",
    "                \n",
    "             \n",
    "                scores = torch.mm(cfeat_norm, dfeat_norm.transpose(0, 1))\n",
    "                scores1 = scores / temperature\n",
    "                \n",
    "                sim_masks = []\n",
    "                for k in range(batch_size):\n",
    "                    mask1 = torch.logical_and((batch['class'] == batch['class'][k]), torch.BoolTensor((np.array(batch['cell_name']) == batch['cell_name'][k])).to(self.device_name))\n",
    "                    mask2 = torch.BoolTensor((np.array(batch['drug_name']) == batch['drug_name'][k])).to(self.device_name)\n",
    "                    mask1[k] = 0\n",
    "                    mask2[k] = 0\n",
    "                    sim_masks.append(mask1.reshape((1, -1)) + mask2.reshape((1, -1)))\n",
    "                sim_masks = torch.cat(sim_masks, dim = 0).to(self.device_name)\n",
    "                scores1.data.masked_fill_(sim_masks, -float('inf'))\n",
    "                \n",
    "                \n",
    "                neg_masks = []\n",
    "                for k in range(batch_size):\n",
    "                    mask1 = torch.rand(batch_size, device = self.device_name) < (1 - (float(num_neg) / batch_size))\n",
    "                    mask1[k] = 0\n",
    "                    neg_masks.append(mask1.reshape((1, -1)))\n",
    "                neg_masks = torch.cat(neg_masks, dim = 0).to(self.device_name)\n",
    "                scores1.data.masked_fill_(neg_masks, -float('inf'))\n",
    "                \n",
    "                scores2 = scores1.transpose(0, 1)\n",
    "                labels = Variable(torch.LongTensor(range(batch_size))).to(self.device_name)\n",
    "             \n",
    "                \n",
    "                \n",
    "                \n",
    "                similarity_loss1 = sim_loss(scores1, labels)\n",
    "                similarity_loss2 = sim_loss(scores2, labels)\n",
    "                \n",
    "\n",
    "                C_loss = 0.9 * similarity_loss1 + 0.1 * similarity_loss2 \n",
    " \n",
    "                C_solver.zero_grad()\n",
    "                C_loss.backward()\n",
    "        \n",
    "                C_solver.step()\n",
    "                \n",
    "                C_losses.append(C_loss.detach().cpu().numpy())\n",
    "                \n",
    "                \n",
    "                if i == max_step:\n",
    "                    break\n",
    "\n",
    "            \n",
    "            t= int(len(np.array(C_losses)) / 1)\n",
    "            x = []\n",
    "            for i in range(int(len(np.array(C_losses)) / t)):\n",
    "                x.append(np.mean(np.array(C_losses)[i*t:(i+1)*t]))\n",
    "            fin_c_losses += x\n",
    "            \n",
    "            train_config['current_epoch'] = epoch\n",
    "            ckpt_dict = {\n",
    "            'condition_state_dict': self.condition_encoder.state_dict(),\n",
    "            'dencoder_state_dict': self.drug_encoder.state_dict(),\n",
    "            'cond_mapper_state_dict': self.cond_mapper.state_dict(),\n",
    "            'drug_mapper_state_dict': self.drug_mapper.state_dict(),\n",
    "            'csolver_state_dict': C_solver.state_dict(),\n",
    "            'C_losses': fin_c_losses,\n",
    "            'configs' : train_config\n",
    "            }\n",
    "\n",
    "        ## Use here to save the model\n",
    "        #torch.save(ckpt_dict, \"../DAS_DATA/model_ckpts/reproduce_10/seed_\"+str(seed)+\"_0914_%d.pth\"%(epoch))\n",
    "              \n",
    "      \n",
    "        return None\n",
    "    \n",
    "  \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aed3815-65df-4efb-9447-eb8e1c330bbe",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfebc3f1-040b-452f-8c78-2f09a5bfa72f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "PREDIFINED_GENOTYPES = ['mut', 'cna', 'cnd']\n",
    "\n",
    "\n",
    "\n",
    "nci_data = pd.read_csv(\"data/drug_response_data/CC_drug_response.csv\")\n",
    "nci_data = nci_data.dropna()\n",
    "\n",
    "valid_celllines = ['TK10_KIDNEY', 'OVCAR5_OVARY', 'HOP92_LUNG', 'SKMEL2_SKIN', 'HS578T_BREAST']\n",
    "\n",
    "nci_data_train = nci_data[~nci_data['ccle_name'].isin(valid_celllines)]\n",
    "nci_data_val = nci_data[nci_data['ccle_name'].isin(valid_celllines)]\n",
    "\n",
    "\n",
    "\n",
    "cell2mut = pd.read_csv(\"data/drug_response_data/original_cell2mut.csv\", index_col = 0).rename(columns={'index':'ccle_name'})\n",
    "cell2cna = pd.read_csv(\"data/drug_response_data/original_cell2cna.csv\", index_col = 0).rename(columns={'index':'ccle_name'})\n",
    "cell2cnd = pd.read_csv(\"data/drug_response_data/original_cell2cnd.csv\", index_col = 0).rename(columns={'index':'ccle_name'})\n",
    "\n",
    "\n",
    "drug2smi = pd.read_csv(\"data/drug_response_data/CC_drug2smi.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0213265-db93-410c-a994-d400d44aa3bc",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7dbebf-a664-43e7-8ee1-f620a597a9cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 44\n",
    "set_seed(seed)\n",
    "train_config = {}\n",
    "train_config['lr'] = 5e-5\n",
    "train_config['batch_size'] = 128\n",
    "train_config['epoch'] = 53\n",
    "train_config['max_step'] = 2000\n",
    "train_config['num_neg'] = 10\n",
    "train_config['warmup_epoch'] = 2\n",
    "train_config['cons_temp'] = 0.3\n",
    "train_config['seed'] = seed\n",
    "\n",
    "device = \"cuda:0\"\n",
    "framework = G2D_DIFF_CE_Pretrain(num_of_genotypes = 3, num_of_dcls = 5, cond_dim = 128,  device = device)\n",
    "framework.to(device).to(torch.float)\n",
    "\n",
    "dataset_obj = GenoDrugDataset(nci_data_train, cell2mut, drug2smi, cna=cell2cna, cnd=cell2cnd)\n",
    "collate_fn = GenoDrugCollator(genotypes=PREDIFINED_GENOTYPES)\n",
    "\n",
    "class_count = []\n",
    "for i in range(5):\n",
    "    class_count.append(len(nci_data_train[nci_data_train['auc_label']==i]))\n",
    "class_count = np.array(class_count)\n",
    "weight = ( 1. / class_count ) \n",
    "samples_weight = np.array([weight[t] for t in nci_data_train['auc_label']])\n",
    "samples_weight = torch.from_numpy(samples_weight) \n",
    "\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))\n",
    "\n",
    "framework.train(dataset_obj = dataset_obj, collate_fn = collate_fn, \\\n",
    "                                 train_config = train_config, sampler = sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7d0643",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_env",
   "language": "python",
   "name": "gen_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
